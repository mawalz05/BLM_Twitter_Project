{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "cwd = os.getcwd()\n",
    "import time\n",
    "\n",
    "# Importing the BLM data (after downloading to your local pc)\n",
    "df = pd.read_csv(r\"C:\\Users\\mawal\\OneDrive\\Desktop\\Twitter\\Final_df\\df_BLM.csv\", encoding = \"ISO-8859-1\")\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords, etc then tokenize each tweet. \n",
    "#Then append each tokenized word to create a single list of all tweets (or unique words?)\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "import emoji\n",
    "\n",
    "# Recoding the start time of the cell\n",
    "start_run = time.time()\n",
    "local_time = time.ctime(start_run)\n",
    "print('start time: {}'.format(local_time))\n",
    "\n",
    "# Converting the tweets from the text column into a single list\n",
    "text_list = df['text'].tolist()\n",
    "\n",
    "# Turning any punction into a period.\n",
    "text_list2 = [re.sub(r'[,!?;-]+', '.', word) for word in text_list]\n",
    "\n",
    "# Tokenizing the words into words in list\n",
    "text_list3 = [nltk.word_tokenize(word) for word in text_list2]\n",
    "\n",
    "# Flattening out the sublists to be a single list\n",
    "text_list_sing = [i for j in text_list3 for i in j]\n",
    "\n",
    "# Creating lists of lists\n",
    "# Removing stopwords and non alpha characters from tokens\n",
    "text_list4 = [[word.lower() for word in sentence if word.isalpha()\n",
    "                or word == '.' or emoji.get_emoji_regexp().search(word)\n",
    "                or word not in stopwords.words('english')] for sentence in text_list3]\n",
    "\n",
    "# Removing stopwords and non alpha characters from tokens in single list\n",
    "text_list_sing = [word.lower() for word in text_list_sing if word.isalpha()\n",
    "                or word == '.' or emoji.get_emoji_regexp().search(word)\n",
    "                or word not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "# Lemmatizing the tokens\n",
    "lemma = WordNetLemmatizer()\n",
    "text_list5 = [[lemma.lemmatize(word) for word in sentence] for sentence in text_list4]\n",
    "text_list_sing = [lemma.lemmatize(word) for word in text_list_sing]\n",
    "\n",
    "\n",
    "# Creating an indexed vocab list\n",
    "vocab_indexed = {'__PAD__':0, '__</e>__': 1, '__UNK__':2}\n",
    "for word in text_list_sing:\n",
    "    if word not in vocab_indexed:\n",
    "        vocab_indexed[word] = len(vocab_indexed)\n",
    "    \n",
    "\n",
    "# Creating the frequency distribution of words\n",
    "fdist = nltk.FreqDist(word for word in text_list_sing)\n",
    "\n",
    "# Creating the vocabulary of unique words\n",
    "vocab = list(set(text_list_sing))\n",
    "\n",
    "# Calculaing the end time and total run time\n",
    "end_run = time.time()\n",
    "local_time = time.ctime(end_run)\n",
    "print('end time: {}'.format(local_time))\n",
    "duration_run = round((end_run - start_run)/60, 2)\n",
    "print('Total run time: {}'.format(duration_run))\n",
    "\n",
    "print('size of the vocabulary: {}'.format(len(vocab)))\n",
    "\n",
    "fdist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using text_list5 to create embeddings\n",
    "sent = text_list5\n",
    "\n",
    "# Using gensims phrases to create bigrams from text_list5\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "phrases = Phrases(sent, min_count = 5, progress_per = 1000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "scores = multiprocessing.cpu_count()\n",
    "model = Word2Vec(min_count = 5, window = 5, size = 300, sample = 6e-5,\n",
    "                   alpha = 0.03, min_alpha = 0.0007, negative = 20, workers = scores-1)\n",
    "\n",
    "# https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n",
    "# The parameters:\n",
    "# min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "# window = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n",
    "# size = int - Dimensionality of the feature vectors. - (50, 300)\n",
    "# sample = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n",
    "# alpha = float - The initial learning rate - (0.01, 0.05)\n",
    "# min_alpha = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
    "# negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "# workers = int - Use these many worker threads to train the model (=faster training with multicore machines)\n",
    "\n",
    "model.build_vocab(sentences, progress_per = 1000)\n",
    "\n",
    "model.train(sentences, total_examples = model.corpus_count, epochs = 30, report_delay = 1)\n",
    "\n",
    "print(model.wv.vocab)\n",
    "\n",
    "model.init_sims(replace = True)\n",
    "\n",
    "print(model.wv.most_similar(positive = ['black']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model.wv.save_word2vec_format('model.bin', binary = True)\n",
    "reloaded_vectors = KeyedVectors.load_word2vec_format('model.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

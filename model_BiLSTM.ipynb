{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# The file can be downloaded from my website and saved locally\n",
    "df = pd.read_csv(r\"C:\\Users\\mawal\\OneDrive\\Desktop\\training.1600000.processed.noemoticon.csv\", \n",
    "                 encoding = \"ISO-8859-1\",\n",
    "                names = ['y', 'id', 'date', 'notsure','handle','text'])\n",
    "\n",
    "# 2 is a neutral sentiment\n",
    "df = df[df['y'] != 2]\n",
    "\n",
    "# The target vector is 0 and 4 so we are dividing by 4 to get 0 and 1\n",
    "df['y'] = df['y']/4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many observations we have\n",
    "one = df[df['y'] == 1]\n",
    "zeroes = df[df['y'] == 0]\n",
    "\n",
    "print(one.shape)\n",
    "print(zeroes.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the data and creating a test sample that can be run instead of the full sample\n",
    "df_test = df.sample(frac = 1)\n",
    "df_test = df_test[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating target and feature vectors\n",
    "# target = df_test['y']\n",
    "# features = df_test['text']\n",
    "target = df['y']\n",
    "features = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = .05, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the shape of the test file and making sure the targets are shuffled\n",
    "print(y_test)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "import emoji\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Recoding the start time of the cell\n",
    "start_run = time.time()\n",
    "local_time = time.ctime(start_run)\n",
    "print('start time: {}'.format(local_time))\n",
    "\n",
    "# Converting the tweets from the text column into a single list\n",
    "text_list_train = X_train.tolist()\n",
    "text_list_test = X_test.tolist()\n",
    "\n",
    "# Turning any punction into a period.\n",
    "text_list2 = [re.sub(r'[,!?;-]+', '.', word) for word in text_list_train]\n",
    "text_list_test2 = [re.sub(r'[,!?;-]+', '.', word) for word in text_list_test]\n",
    "\n",
    "# Removing stopwords and non alpha characters from tokens\n",
    "text_list3 = [word.lower() for word in text_list2 if word.isalpha()\n",
    "                or word == '.' or emoji.get_emoji_regexp().search(word)\n",
    "                or word not in stopwords.words('english')]\n",
    "\n",
    "text_list_test3 = [word.lower() for word in text_list_test2 if word.isalpha()\n",
    "                or word == '.' or emoji.get_emoji_regexp().search(word)\n",
    "                or word not in stopwords.words('english')]\n",
    "\n",
    "# Lemmatizing the tokens\n",
    "# lemma = WordNetLemmatizer()\n",
    "# text_list4 = [lemma.lemmatize(word) for word in text_list3]\n",
    "\n",
    "# Tokenizing using the tensorflow function so we can use <OOV>\n",
    "tokenizer = Tokenizer(oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_list3)\n",
    "\n",
    "# Creating a word index where each word in the corpus is indexed starting from 0\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Using the tokenizer to generate sequences that are the token indices in place of the tokens\n",
    "sequences = tokenizer.texts_to_sequences(text_list3)\n",
    "sequences_test = tokenizer.texts_to_sequences(text_list_test3)\n",
    "\n",
    "# Adding padding (post) to make all of the tweet lengths the same (cut the length at 20 tokens)\n",
    "padded_sequences = pad_sequences(sequences, maxlen = 40, padding = 'post')\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = 40, padding = 'post')\n",
    "\n",
    "# tokenizer.fit_on_texts(text_list_test3)\n",
    "# word_index_test = tokenizer.word_index\n",
    "# sequences_test = tokenizer.texts_to_sequences(text_list_test3)\n",
    "# padded_sequences_test = pad_sequences(sequences_test, maxlen = 40, padding = 'post')\n",
    "\n",
    "# Calculaing the end time and total run time\n",
    "end_run = time.time()\n",
    "local_time = time.ctime(end_run)\n",
    "print('end time: {}'.format(local_time))\n",
    "duration_run = round((end_run - start_run)/60, 2)\n",
    "print('Total run time: {}'.format(duration_run))\n",
    "\n",
    "print('size of the vocabulary: {}'.format(len(word_index)))\n",
    "print('First 10 words and their indices: {}'.format(list(word_index.items())[:10]))\n",
    "print('First five seqeunces: {}'.format(padded_sequences[:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing some diagnostic tests\n",
    "print(len(word_index))\n",
    "print(len(text_list_train))\n",
    "print(len(text_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further looks at the data\n",
    "print(list(word_index.items())[:50])\n",
    "print(padded_sequences_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# We can save the training and validation word embeddings so we can uplaod them in the future\n",
    "with h5py.File('padded_sequences.h5', 'w') as hf:\n",
    "    hf.create_dataset('padded_sequences', data = padded_sequences)\n",
    "    \n",
    "with h5py.File('y_train.h5', 'w') as hf:\n",
    "    hf.create_dataset('y_train', data = y_train)\n",
    "    \n",
    "with h5py.File('padded_sequences_test.h5', 'w') as hf:\n",
    "    hf.create_dataset('padded_sequences_test', data = padded_sequences_test)\n",
    "\n",
    "with h5py.File('y_test.h5', 'w') as hf:\n",
    "    hf.create_dataset('y_test', data = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading the training and validation data (if needed)\n",
    "with h5py.File('padded_sequences.h5', 'r') as hf:\n",
    "    X_train_bow = hf['padded_sequences'][:]\n",
    "\n",
    "with h5py.File('y_train.h5', 'r') as hf:\n",
    "    y_train_bow = hf['y_train'][:]\n",
    "    \n",
    "with h5py.File('padded_sequences_test.h5', 'r') as hf:\n",
    "    X_val_bow = hf['padded_sequences_test'][:]\n",
    "    \n",
    "with h5py.File('y_test.h5', 'r') as hf:\n",
    "    y_val_bow = hf['y_test'][:]\n",
    "    \n",
    "    \n",
    "print(X_train_bow.shape)\n",
    "print(X_val_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Generating the BiLSTM model.\n",
    "model2 = keras.Sequential([layers.Embedding(len(word_index) + 1, 32),\n",
    "                         layers.Bidirectional(layers.LSTM(32, dropout = .5)),\n",
    "                         layers.Dropout(.5),\n",
    "                         layers.Dense(16, activation = 'relu'),\n",
    "                         layers.Dropout(.5),\n",
    "                         layers.Dense(1, activation = 'sigmoid')])\n",
    "\n",
    "model2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "history = model2.fit(padded_sequences, y_train, batch_size = 16, epochs = 3, validation_data = (padded_sequences_test, y_test),\n",
    "                    validation_steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot the data to see the training and validation error rates over epochs\n",
    "# Turning the model into a dictionary\n",
    "history_dict = history.history\n",
    "\n",
    "# Extracting the accuracy of the model\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "epochs = range(1, len(acc) +1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epochs, acc, 'bo', label = 'Training acc') \n",
    "plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc= 'lower right')\n",
    "plt.ylim((0.5, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# We can view the word embeddings in a word cloud through tensorflow embedding projector\n",
    "# https://projector.tensorflow.org/\n",
    "\n",
    "out_vectors = io.open('vecs.tsv', 'w', encoding = 'utf-8')\n",
    "out_metadata = io.open('meta.tsv', 'w', encoding = 'utf-8')\n",
    "\n",
    "vocab = list(word_index.keys())\n",
    "\n",
    "weights = model2.layers[0].get_weights()[0] #0 layer is the embedding layer\n",
    "for num, word in enumerate(vocab):\n",
    "    vec = weights[num+1]\n",
    "    out_metadata.write(word + '\\n')\n",
    "    out_vectors.write('\\t'.join([str(x) for x in vec]) + '\\n')\n",
    "out_vectors.close()\n",
    "out_metadata.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the unlabelled BLM data\n",
    "df = pd.read_csv(r\"C:\\Users\\mawal\\OneDrive\\Desktop\\Twitter\\Final_df\\df_BLM.csv\", encoding = \"ISO-8859-1\")\n",
    "\n",
    "# Extracting the tweets from the dataframe and creating a list\n",
    "df_text = df['text']\n",
    "df_text = df_text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the BLM twitter data to make predictions\n",
    "# Turning any punction into a period.\n",
    "pred_data = [re.sub(r'[,!?;-]+', '.', word) for word in df_text]\n",
    "\n",
    "# Removing stopwords and non alpha characters from tokens\n",
    "pred_data = [word.lower() for word in pred_data if word.isalpha()\n",
    "                or word == '.' or emoji.get_emoji_regexp().search(word)\n",
    "                or word not in stopwords.words('english')]\n",
    "\n",
    "# Using the tokenizer to generate sequences that are the token indices in place of the tokens\n",
    "pred_data = tokenizer.texts_to_sequences(pred_data)\n",
    "\n",
    "# Adding padding (post) to make all of the tweet lengths the same (cut the length at 40 tokens)\n",
    "pred_data = pad_sequences(pred_data, maxlen = 40, padding = 'post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating sentiment predictions\n",
    "sentiment = model2.predict_proba(pred_data, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sentiment probability column and classifier column in the dataframe\n",
    "df['sentiment_probs'] = sentiment\n",
    "df['sentiment'] = np.where(df['sentiment_probs'] > .5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the predictions to local drive\n",
    "df.to_csv(r'C:\\Users\\mawal\\OneDrive\\Desktop\\Twitter\\predictions\\BLM_labelled.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
